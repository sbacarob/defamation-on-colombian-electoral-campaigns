{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping data from Twitter\n",
    "\n",
    "If you prefer to use Twitter's API to obtain the data you'll find steps to do so further down. However, Twitter's API limitations don't allow for extracting much data.\n",
    "\n",
    "Hence, I've created a script called `extractor.py` that works with Selenium and BeautifulSouop to extract the data and it will work without any API keys. You will, however, need to install the Selenium package for Python and download the driver you wish to use. My script runs using Firefox with [geckodriver](https://github.com/mozilla/geckodriver/releases)\n",
    "\n",
    "So, you can generate a better dataset using this than the free version of Twitter's API. There's a tradeoff regarding the amount of time it takes to retrieve the data.\n",
    "\n",
    "## Retrieving the data using the script\n",
    "\n",
    "### 1. Install the Selenium package for Python\n",
    "\n",
    "If you're using Conda, it'll be enough to run `conda install selenium`, otherwise, you can install it with pip using `pip install selenium`.\n",
    "\n",
    "### 2. Download a driver for a browser\n",
    "\n",
    "As mentioned above, my version of the script works with Firefox using the geckodriver, which you can download from [here](https://github.com/mozilla/geckodriver/releases), depending on the version of the browser you have installed.\n",
    "\n",
    "You can also run the browser in headless mode, meaning that you won't see the driver performing tasks. It will do it invisibly. You can read about how to do that in Selenium docs.\n",
    "\n",
    "### 3. Add the driver to your `PATH`\n",
    "\n",
    "On Linux based systems, you can achieve this by running:\n",
    "\n",
    "```bash\n",
    "$ export PATH=\"/path/to/driver/dir:$PATH\"\n",
    "```\n",
    "\n",
    "> **Note:** this will only keep the path in your `PATH` for the current terminal session. You can look for ways to add it in a definitive way if you so wish.\n",
    "\n",
    "### 4. Import the script and use its functions:\n",
    "\n",
    "> **Update:** After running this method for a couple of times Twitter seems to have blocked my IP for the specific queries I was running. Therefore, I've added a little extra step, which is using a proxy server. This forced me to switch to Firefox, because Chrome wasn't loading the websites using the proxy server. So, you should add a host for a proxy server before using this. You can easily find proxy servers by just running a Google search for 'Fresh proxies' or something like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import twitter_data_extractor as td\n",
    "\n",
    "# This will take a good while\n",
    "duque_data = td.download_data_for_period('@IvanDuque', '2018-01-27', '2018-06-16', daily_limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('data/duque_scraped_data.json', 'w') as f:\n",
    "    json.dump(duque_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will load about 14 thousand tweets mentioning @IvanDuque. You can tweak the search parameters. Changing the interval to query, or the number of tweets to extract per day. The extractor will iterate over the dates in the interval, running a query on Twitter, and return a list of tweets at the end, taking about `daily_limit` results from each day. This won't necessarily be equal to `daily_limit`, but always at least `daily_limit`, depending on the amount of tweets Twitter loads each time a scroll is performed.\n",
    "\n",
    "Then, you can load the data for the other candidate in the same fashion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "petro_data = td.download_data_for_period('@petrogustavo', '2018-01-27', '2018-06-16', daily_limit=100)\n",
    "\n",
    "with open('data/petro_scraped_data.json', 'w') as f:\n",
    "    json.dump(petro_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** If you don't pass the `daily_limit` parameter, all tweets found for the period will be returned.\n",
    "\n",
    "And finally, you can concatenate the results and store them in a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_data = duque_data + petro_data\n",
    "\n",
    "with open('data/scraped_data.json', 'w') as f:\n",
    "    json.dump(whole_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! You've now got almost 30 thousand tweets to work with.\n",
    "\n",
    "If you're just here to load the data, you don't need to continue reading, unless you prefer to load the data from Twitter's API.\n",
    "\n",
    "Below are the necessary steps to retrieve data from Twitter's API if you prefer to use that. However, I would recommend using the extractor, if you've got the patience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving the data from Twitter's API\n",
    "\n",
    "This were the steps taken to retrieve data from Twitter, mentioning the candidates Iván Duque and Gustavo Petro.\n",
    "\n",
    "This was done using Twitter's HTTP API with the requests package.\n",
    "\n",
    "You can apply for a developer account [here](https://developer.twitter.com/en/apply-for-access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "consumer_key = \"\" # Set this to your consumer key\n",
    "consumer_secret = \"\" # Set this to your consumer secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Twitter API\n",
    "\n",
    "There are Python packages that allow access to Twitter's APIs, however, for this project, they will be left aside, so that the data can be used exactly in the way it's needed.\n",
    "\n",
    "Therefore, the `requests` package will be used to perform the requests.\n",
    "\n",
    "### Authenticating requests\n",
    "\n",
    "Twitter requires that the requests to its APIs are authenticated. The whole authentication process is described in detail [here](https://developer.twitter.com/en/docs/basics/authentication/overview/application-only), but it basically consists of 2 steps:\n",
    "\n",
    "1. Base 64 encoding the consumer key and secret\n",
    "2. Obtaining an access token from Twitter's Oauth2 endpoint\n",
    "\n",
    "So, here's a very simple way to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded = base64.b64encode(\"%s:%s\" % (consumer_key, consumer_secret))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have succesfully base64 encoded the consumer key and secret.\n",
    "\n",
    "Now, it's necessary to obtain an access token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'Authorization': 'Basic ' + encoded, # An auth header needs to be sent, with the encoded string from before\n",
    "    'Content-Type': 'application/x-www-form-urlencoded;charset=UTF-8' # This is the required content type\n",
    "}\n",
    "\n",
    "# Twitter's docs explain that this must be the body sent to the Oauth2 endpoint\n",
    "req_body = 'grant_type=client_credentials'\n",
    "\n",
    "# This is just the URL for the Oauth2 endpoint\n",
    "auth_url = 'https://api.twitter.com/oauth2/token'\n",
    "\n",
    "# We send a POST request to the endpoint, passing the body and headers\n",
    "auth_token_res = requests.post(auth_url, data=req_body, headers=headers)\n",
    "\n",
    "# We extract the response, which is a JSON object\n",
    "auth_object = auth_token_res.json()\n",
    "\n",
    "# We extract the access token from the response\n",
    "access_token = auth_object['access_token']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there it is!\n",
    "\n",
    "We've succesfully gotten an auth token that we'll use for requesting the data from now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to start making requests to Twitter's API, and since we'll be using the same auth token and URL, we'll set those next\n",
    "\n",
    "### Twitter's API URLs\n",
    "\n",
    "Twitter has an interesting way for generating query endpoints for users to access their API. It looks like this:\n",
    "\n",
    "```\n",
    "                                               And this label refers to the label you chose\n",
    " This looks the same for everybody              for your environment. Mine is 'Development'\n",
    "|----------------------------------------|            |\n",
    "https://api.twitter.com/1.1/tweets/search/:product/:label.json\n",
    "                                              |\n",
    "                        This refers to the \"product\". e.g. 30day or fullarchive\n",
    "                                     in the case of search\n",
    "```\n",
    "\n",
    "For the auth token, we'll use an `Authorization` header with a value of `\"Bearer <token>\"`, where `<token>` is of course, the access token we obtained before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_api_url = 'https://api.twitter.com/1.1/tweets/search/30day/Development.json'\n",
    "\n",
    "headers = {\n",
    "    'Authorization': 'Bearer ' + access_token\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requesting data from the API\n",
    "\n",
    "Now, we can start issuing GET requests to the endpoint, adjusting the `'query'` param according to our needs.\n",
    "For the case of this project, only two things will be included in the query, a \"mention\" of the user we'll be targeting, and we would also like to exclude retweets.\n",
    "\n",
    "The \"mention\" part is pretty straightforward, it only needs to be included in the query with an '@' sign.\n",
    "\n",
    "Twitter provides a couple of ways for excluding retweets, namely, you can use `is:retweet` or `filter:retweet`, which you would need to include with a \"`-`\" sign to specify negation. Like `-is:retweet`.\n",
    "\n",
    "However, those operators are not available on the Sandbox version, which is the free version of premium endpoints, like the full archive search or 30 day search ones. So, to workaround this issue, we can negate the exact match \"RT\", because we know that all retweets will include this right at the start. This might lead to a couple of false negatives, in case a user had \"RT\" as part of their tweet, but since the text of tweets is tokenized, we don't have to worry about tweets including words like \"ART\" being ommitted, because RT will be searched as a whole \"word\".\n",
    "\n",
    "Also, the fromDate and toDate params will be used when getting the actual data for the project, to specify the dates of the electoral campaigns\n",
    "\n",
    "With all that said, we can now build our query and start getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'query': '@IvanDuque -\"RT\"'\n",
    "}\n",
    "\n",
    "data = requests.get(base_api_url, headers=headers, params=params)\n",
    "\n",
    "data = data.json()\n",
    "\n",
    "results = data['results']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've gotten quite a good amount of information. Each of these requests will return 100 tweets, and there is a `'next'`, field, which contains a token we can use for retrieving the next 100 tweets for the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data limitations\n",
    "\n",
    "With the sandbox version of the API, we're limited to only 50 requests to the full archive endpoint, with 100 tweets each. Since the 30day allows some more (250), we'll use it for testing.\n",
    "\n",
    "For this project, we're particularly interested in the period of the 2018 electoral campaign in Colombia. This campaigns ran from january 27, 2018, to may 27, 2018 for the first round of voting. And for the second one, from may 27, 2018, to june 17, 2018. So in general, we're interested in the period between january 27, 2018 and june 17, 2018.\n",
    "\n",
    "That leaves us with 5 months of data we're interested in. So, considering that we're limited to 50 requests, it would be fine to split those equally among the relevant months.\n",
    "\n",
    "We would also like to focus on two candidates in particular: Iván Duque, the elected president of Colombia, and Gustavo Petro, his most noticeable opponent. So, it would be okay to leave half of the monthly requests to each one of the candidates.\n",
    "\n",
    "To summarize, here's how we'll split our requests:\n",
    "\n",
    "|Date / N° of requests  | Gustavo Petro | Iván Duque |\n",
    "|-----------------------| ------------- | ---------- |\n",
    "|2018-01-27 - 2018-02-27| 5             | 5          |\n",
    "|2018-02-27 - 2018-03-27| 5             | 5          |\n",
    "|2018-03-27 - 2018-04-27| 5             | 5          |\n",
    "|2018-04-27 - 2018-05-27| 5             | 5          |\n",
    "|2018-05-27 - 2018-06-17| 5             | 5          |\n",
    "|total                  | 25            | 25         |\n",
    "\n",
    "This will leave us with a total of 2500 tweets about each candidate\n",
    "\n",
    "However, since for now we're only testing, we will use a slightly different variation, working with the 30day index.\n",
    "\n",
    "So, the idea here will be to split the last month in five intervals, and request data at least twice for only one of the candidates. This way we can mimic the process we'll be doing with the data we really want. So let's get to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll import a couple of utilities to get the dates as necessary\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# We set an initial date 30 days ago, and shift it 6 days at a time to get the intervals\n",
    "current_date = datetime.now() - timedelta(days=30)\n",
    "intervals = []\n",
    "\n",
    "for i in range(0, 5):\n",
    "    # Set a 'next' date, 6 days from the current one\n",
    "    next_date = current_date + timedelta(days=6)\n",
    "\n",
    "    interval = {\n",
    "        'fromDate': current_date.strftime(\"%Y%m%d\") + '0000',\n",
    "        'toDate': next_date.strftime(\"%Y%m%d\") + '0000' # format the dates in the way expected by Twitter\n",
    "    }\n",
    "\n",
    "    intervals.append(interval)\n",
    "    current_date = next_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got the intervals, we can start requesting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def request_data(intervals, search_term, requests_per_interval=2):\n",
    "    results = []\n",
    "    \n",
    "    for interval in intervals:\n",
    "        # Params need to be reset for each interval\n",
    "        params = {'query': search_term + ' -\"RT\"'}\n",
    "        # Add the fromDate and toDate fields to the params\n",
    "        params.update(interval)\n",
    "        \n",
    "        # We perform an initial request to ensure that our result will have a 'next' field.\n",
    "        current_request = requests.get(base_api_url, headers=headers, params=params)\n",
    "        \n",
    "        for i in range(0, requests_per_interval):\n",
    "            data = current_request.json()\n",
    "            results += data['results']\n",
    "            if i + 1 < requests_per_interval:\n",
    "                # We add the next param, to fetch the next \"page\" of results\n",
    "                params['next'] = data['next']\n",
    "                current_request = requests.get(base_api_url, headers=headers, params=params)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we fetch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = request_data(intervals, '@IvanDuque')\n",
    "\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! We've now gotten 1000 tweets we can store in a JSON file or other type of file, so that we can later process it and adjust it to our needs\n",
    "\n",
    "## Exporting the tweets\n",
    "\n",
    "We'll now save this data into a new JSON file, to use it in the other parts of the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# the json package provides a simple dump function to encode data as json\n",
    "with open('data/test_data.json', 'w') as f:\n",
    "    json.dump(test_data, f) # here, the json dump is being sent directly to the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done. We now have a file that contains all of our downloaded data.\n",
    "\n",
    "# Working with the actual data\n",
    "\n",
    "Now we're more than ready to repeat the process with the data we're really going to use in our project.\n",
    "\n",
    "We'll still use the headers from before, with the same auth token. So that doesn't need to be changed.\n",
    "\n",
    "The endpoint for the full archive, however, is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You'll notice that my environment here is called 'development' with a lowercase 'd'\n",
    "# and that the endpoint for the full archive is just 'fullarchive'\n",
    "\n",
    "base_api_url = 'https://api.twitter.com/1.1/tweets/search/fullarchive/development.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, for this endpoint, the approach we'll use with the dates will be a little bit different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the relevant dates in our time period of interest\n",
    "dates = [\"201801270000\", \"201802270000\", \"201803270000\", \"201804270000\", \"201805270000\", \"201806172359\"]\n",
    "\n",
    "# Produce intervals between one date and the next\n",
    "intervals = [{'fromDate': dates[i], 'toDate': dates[i + 1]} for i in range(0, len(dates) - 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now request our data just like we did before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll first request data for the candidate Iván Duque\n",
    "duque_data = request_data(intervals, '@IvanDuque', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ande save it to its own file\n",
    "with open('data/duque_data.json', 'w') as f:\n",
    "    json.dump(duque_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now we'll request the data for the candidate Gustavo Petro\n",
    "petro_data = request_data(intervals, '@petrogustavo', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And we save it to its own file as well\n",
    "with open('data/petro_data.json', 'w') as f:\n",
    "    json.dump(petro_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And finally, we'll concatenate both lists and store the result in a single file\n",
    "all_data = duque_data + petro_data\n",
    "\n",
    "with open('data/data.json', 'w') as f:\n",
    "    json.dump(all_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it. Our data files are now ready to be used!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
